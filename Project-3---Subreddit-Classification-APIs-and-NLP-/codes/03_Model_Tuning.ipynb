{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd316b86",
   "metadata": {},
   "source": [
    "# Building, Tuning and Selection of  Models\n",
    "\n",
    "\n",
    "## Business problem:\n",
    "\n",
    "- We want to help the moderators to automate the classification and this will benefit the maintenance and sanity of each subreddits by posting to the correct group.\n",
    "\n",
    "- Can we accurately label if a post is more relevant for *Nutrition* or *Keto* based on their title?\n",
    "\n",
    "## What we have done so far?\n",
    "- Cleaned up our feature which is title to 'preproc_title' (using tokenizer and stemmer)\n",
    "- created is_nutrition, where 1 means posted under r/Nutrition, and 0 as posted from r/Keto\n",
    "\n",
    "***is_Nutrition = 1 *** is our positive case\n",
    "\n",
    "## What are the next steps?\n",
    "\n",
    "1. Load the data\n",
    "    - 1.1 Check the base model\n",
    "\n",
    "2. Define independent(X) and dependent(y)  variable.\n",
    "    - 2.1 Split our data for training and testing.\n",
    "\n",
    "3. Define function and dictionary to store parameters and results from building the model\n",
    "\n",
    "4. Building models and tuning\n",
    "    - 4.1 CountVectorizer and Logistic Regression\n",
    "    - 4.2 TfidfVectorizer and Logistic Regression\n",
    "    - 4.3 CountVectorizer and DecisionTree\n",
    "    - 4.4 CountVectorizer and Naive Bayes    \n",
    "    - 4.5 CountVectorizer and BaggingClassifier\n",
    "    - 4.6 CountVectorizer and RandomForest\n",
    "    - 4.7 CountVectorizer and ADA Boost\n",
    "    - 4.8 CountVectorizer and Gradient Boost\n",
    "    - 4.9 VotingClassifier\n",
    "    - 4.99 CountVectorizer and SVM\n",
    "\n",
    "5. Model Evaluation\n",
    "    - 5.1 Score or Accuracy\n",
    "    - 5.2 Other Metrics ( Sensitivity, Specificity, Precision, F1 Score)\n",
    "\n",
    "6. Challenges\n",
    "     - 6.1 Overfitting Problem\n",
    "\n",
    "7. Selecting our Best Model\n",
    "    - 7.1 MODEL SELECTION\n",
    "    \n",
    "8. AUC-ROC Curve\n",
    "\n",
    "9. POST MODEL ANALYSIS\n",
    "    - 9.1 Distribution of True Value (Y value) and the Prediction (probability)\n",
    "    - 9.2 Confusion Matrix\n",
    "\n",
    "10. Deep dive in misclassified posts\n",
    "\n",
    "11. Coef_\n",
    "\n",
    "12. Summary and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e865e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our packages here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve, plot_confusion_matrix\n",
    "\n",
    "#not used, we used manual calculation\n",
    "#accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#cv, tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "#decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#naivebayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#boosting\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "\n",
    "#svm\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "#set our width to 500, since title are a bit long\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76673b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34a0c",
   "metadata": {},
   "source": [
    "### 1.0 Load the preprocessed and combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db6433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data for both datasets\n",
    "df_subr = pd.read_csv(\"../datasets/combined_subr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e73614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the datasets\n",
    "print(\"Keto & Nutrition\", df_subr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0063680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "print(df_subr.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cb3f4",
   "metadata": {},
   "source": [
    "### 1.1 Baseline Model \n",
    "\n",
    "Our baseline model is 50%, it means that without any models, if we assign all post to Nutrition (1 as positive case), we then have  50% probability of being accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bd2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution\n",
    "df_subr[\"is_nutrition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model\n",
    "df_subr[\"is_nutrition\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a78bf3",
   "metadata": {},
   "source": [
    "### 2.0 Define X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_subr[\"preproc_title\"]\n",
    "y = df_subr[\"is_nutrition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e263cea",
   "metadata": {},
   "source": [
    "### 2.1 Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed099cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use stratify=y to balance the distribution for both class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the distribution\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d8026",
   "metadata": {},
   "source": [
    "### 3.0 Defining pipeline and parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will store all our data in a dictionary for easy storage and access\n",
    "#includes model pipeline, parameters, and results\n",
    "\n",
    "#initialize the dictionary\n",
    "models = {}\n",
    "\n",
    "#our model's main keys\n",
    "model_list = [\"CVEC\", \"TVEC\", \"DTREE\", \"NB\", \"BAG\", \"RF\", \"ADA\", \"GB\", \"VC\", \"SVC\"]\n",
    "\n",
    "\n",
    "for model in model_list:\n",
    "    models[model] = \n",
    "\n",
    "# ******PIPELINES****** #\n",
    "\n",
    "#Logistic Regression with CountVectorizer\n",
    "models[\"CVEC\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())]\n",
    "\n",
    "\n",
    "#Logistic Regression with TfidfVectorizer\n",
    "models[\"TVEC\"][\"pipe\"] = [\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression()) ]\n",
    "\n",
    "\n",
    "#DecisionTree\n",
    "models[\"DTREE\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('dt', DecisionTreeClassifier()) ]\n",
    "\n",
    "#NaiveBayes\n",
    "models[\"NB\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB()) ]\n",
    "\n",
    "#bagging\n",
    "models[\"BAG\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('bag', BaggingClassifier(base_estimator=DecisionTreeClassifier())) ]\n",
    "\n",
    "#RandomForest\n",
    "models[\"RF\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier()) ]\n",
    "\n",
    "#ADA\n",
    "models[\"ADA\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ada', AdaBoostClassifier(base_estimator=DecisionTreeClassifier())) ]\n",
    "\n",
    "#Gradient Boosting\n",
    "models[\"GB\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier()) ]\n",
    "\n",
    "\n",
    "\n",
    "#SVM\n",
    "models[\"SVC\"][\"pipe\"] = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('svm', SVC() )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d4dce",
   "metadata": {},
   "source": [
    "### 3.1 Function to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, withGridSearch=False):\n",
    "    \n",
    "    '''build the model, use the pipeline and paratemer from the model dictionary\n",
    "    pass the model name as initialized in model_list{}\n",
    "    withGridSearch=False (will fit using the pipeline only)\n",
    "    withGridSearch=True (will use the GridSearch and hyperparameters)\n",
    "    '''\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Building Model.. {model}   Current Time = {current_time}\")\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    \n",
    "    if withGridSearch:\n",
    "        gsearch = \"gridsearch\"\n",
    "    else:\n",
    "        gsearch = \"no_gridsearch\"\n",
    "        \n",
    "    models[model][gsearch] = {}\n",
    "    \n",
    "    pipeL = Pipeline(models[model][\"pipe\"])\n",
    "    \n",
    "    if withGridSearch:\n",
    "         gs = GridSearchCV(pipeL, models[model][\"params\"], cv=kfold)\n",
    "    else:\n",
    "        gs = pipeL\n",
    " \n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    #add information on best params and score\n",
    "    models[model][gsearch][\"train_score\"] = gs.score(X_train, y_train)\n",
    "    models[model][gsearch][\"test_score\"] = gs.score(X_test, y_test)\n",
    "    models[model][gsearch][\"model\"] = gs\n",
    "    \n",
    "    if withGridSearch:\n",
    "        models[model][gsearch][\"best_params_\"] = gs.best_params_\n",
    "    else:\n",
    "        models[model][gsearch][\"best_params_\"] = \"n/a\"\n",
    "\n",
    "    predictions = models[model][gsearch][\"model\"].predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    \n",
    "    #calculation if we use the package from the metrics, but it's no fun, right?\n",
    "    \n",
    "    #recall = recall_score(y_test, predictions)\n",
    "    #precision = precision_score(y_test, predictions)\n",
    "    #f1 = f1_score(y_test, predictions)\n",
    "    \n",
    "    #let's calculate by hand\n",
    "    sensitivity = tp / (tp + fn) \n",
    "    specificity = tn / (tn + fp)\n",
    "    precision =  tp / (tp + fp)\n",
    "    f1_score = 2 * ((precision * sensitivity) / (precision + sensitivity))\n",
    "    \n",
    "    \n",
    "    models[model][gsearch][\"sensitivity\"] = sensitivity\n",
    "    models[model][gsearch][\"specificity\"] = specificity\n",
    "    models[model][gsearch][\"precision\"] =  precision\n",
    "    models[model][gsearch][\"f1_score\"] = f1_score\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Model Completion.. {model}   Current Time = {current_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c9788",
   "metadata": {},
   "source": [
    "## 4. Building and Tuning the MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e628cfd",
   "metadata": {},
   "source": [
    "### 4.1 CountVectorizer and Logistic Regression\n",
    "***CountVectorizer*** transform Title into a bag of words (in a simple term), or a vector on the basis of the frequency or count of each word that occurs in the entire Title (as a technical description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f46727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "\n",
    "build_model(\"CVEC\", withGridSearch=False)\n",
    "display(models[\"CVEC\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29076908",
   "metadata": {},
   "source": [
    "#### Hyperparametes -- CountVectorizer (from sklearn)\n",
    "\n",
    "from sklearn:\n",
    "***max_df*** default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). \n",
    "If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "***min_df**  default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "***max_features*** default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters\n",
    "models[\"CVEC\"][\"params\"] = {\n",
    "    'cvec__stop_words':[None, \"english\"],\n",
    "    'cvec__max_features': [2000,3000,4000,10000],\n",
    "    'cvec__min_df': [1, 2,3,],\n",
    "    'cvec__ngram_range' : [(1,1), (1,2),(2,2)],\n",
    "    'cvec__max_df': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "build_model(\"CVEC\", True)\n",
    "display(models[\"CVEC\"][\"gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe056f4",
   "metadata": {},
   "source": [
    "*Looks like our model with default parameters is slighly better (and faster) than the GridSearch model* We also notice overfitting, as the train score is over 90% and test score is below 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e9091",
   "metadata": {},
   "source": [
    "### 4.2 TfidfVectorizer and Logistic Regression\n",
    "\n",
    "***TfidfVectorizer*** multiplies two metrics: (1) how many times a word appears in a document (2) the inverse document frequency of the word across a set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "build_model(\"TVEC\", withGridSearch=False)\n",
    "display(models[\"TVEC\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check valid parameters that we can define\n",
    "tf = TfidfVectorizer()\n",
    "tf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters, min_df, max_df is the same definition as CountVectorizer\n",
    "\n",
    "models[\"TVEC\"][\"params\"] =  {\n",
    "    \"tvec__max_features\" : [4000,5000],\n",
    "    \"tvec__ngram_range\" : [(1,1), (1,2),(2,2)],\n",
    "    \"tvec__stop_words\" : [None, \"english\"],\n",
    "    'tvec__min_df':[1,2, 3],\n",
    "    'tvec__max_df': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "\n",
    "build_model(\"TVEC\", True)\n",
    "display(models[\"TVEC\"][\"gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f69d8",
   "metadata": {},
   "source": [
    "*Tuning the ***TfidfVectorizer*** model is slightly better. The score is also higher than the ***CountVectorizer***.* There is also evidence of  overfitting for both models, as the train score is over 90% and test score is around 81-82%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff62618",
   "metadata": {},
   "source": [
    "### 4.3 CountVectorizer and DecisionTree\n",
    "\n",
    "***DecisionTree*** The classification process is easy to visualize and understand. It uses a set of rules to make decisions. Just like a flowchart diagram with the terminal nodes representing classification or decisions. Starting with a dataset, you can measure the entropy to find a way to split the set until all the data belongs to the same class. \n",
    "\n",
    "*Note on Decision Tree: default parameters (as per sklearn documentation)*\n",
    "- The default values for the parameters controlling the size of the trees (e.g. max_depth, min_samples_leaf, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "\n",
    "build_model(\"DTREE\", withGridSearch=False)\n",
    "display(models[\"DTREE\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba73a1",
   "metadata": {},
   "source": [
    "#### Hyperparameters - DecisionTreeClassifier (from sklearn)\n",
    "\n",
    "***max_depth***, default=None\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "***min_samples_split***, default=2\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "\n",
    "***min_samples_leaf*** , default=1\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "***ccp_alpha***, default=0.0\n",
    "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfede27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters    \n",
    "models[\"DTREE\"][\"params\"]  = {\n",
    "    #cvec__ parameters here comes from the best_params_ from the GridSearch\n",
    "    'cvec__max_df': [0.2],\n",
    "    'cvec__max_features': [10000],\n",
    "    'cvec__min_df': [1],\n",
    "    'cvec__ngram_range': [(1, 2)],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    #DecisionTree parameter\n",
    "    'dt__random_state':[0],\n",
    "    'dt__max_depth':[100, 125, 150], \n",
    "    'dt__min_samples_split': [25,50,100],\n",
    "    'dt__min_samples_leaf': [2, 3,4],\n",
    "    'dt__ccp_alpha' : [0.01, 0]\n",
    "}\n",
    "build_model(\"DTREE\", True)\n",
    "display(models[\"DTREE\"][\"gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8ef99",
   "metadata": {},
   "source": [
    "*GridSearch model is not overfitting as much as the model with default parameter and has higher score as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a623149",
   "metadata": {},
   "source": [
    "### 4.3 Naive Bayes\n",
    "***Naive Bayes*** methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "\n",
    "***MultinomialNB*** implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d084c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "build_model(\"NB\", withGridSearch=False)\n",
    "display(models[\"NB\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43755f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"NB\"][\"params\"] =  {\n",
    "                          'nb__fit_prior': [True, False],\n",
    "                          'nb__alpha': [0, 0.4, 0.8]\n",
    "                    }\n",
    "\n",
    "#build with hyperparameter\n",
    "build_model(\"NB\", withGridSearch=True)\n",
    "display(models[\"NB\"][\"gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dc002",
   "metadata": {},
   "source": [
    "*NaiveBayes using GridSearch performs better, the overfitting improved as well using GridSearch.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16294a",
   "metadata": {},
   "source": [
    "### 4.5 CountVectorizer and BaggingClassifier\n",
    "***BaggingClassifier***  is a bootstrap aggregating.\n",
    "***Bootstrapping*** means random resampling with replacement. \n",
    "Bagging (bootstrap aggregating) mitigates the overfitting problem by exposing different trees to different sub-samples of the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "build_model(\"BAG\", withGridSearch=False)\n",
    "display(models[\"BAG\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just checkign what are valid hyperparameters for BaggingClassifier\n",
    "bg = BaggingClassifier()\n",
    "bg.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55bc6c1",
   "metadata": {},
   "source": [
    "#### Hyperparameters - BaggingClassfier (from sklearn)\n",
    "\n",
    "***n_estimators***, default=10\n",
    "The number of base estimators in the ensemble.\n",
    "\n",
    "***max_samples***, default=1.0\n",
    "The number of samples to draw from X to train each base estimator (with replacement by default)\n",
    "\n",
    "***max_features***, default=1.0\n",
    "The number of features to draw from X to train each base estimator ( without replacement by default, see bootstrap_features for more details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters\n",
    "\n",
    "models[\"BAG\"][\"params\"]  ={    \n",
    "    #**cvec__ parameters here comes from the best_params_ from the GridSearch, \n",
    "    #**however, the data is performing poorly so might as well use the default value\n",
    "    \n",
    "    #'cvec__max_df': [0.2],\n",
    "    #'cvec__max_features': [10000],\n",
    "    #'cvec__min_df': [1],\n",
    "    #'cvec__ngram_range': [(1, 2)],\n",
    "    #'cvec__stop_words': ['english'],\n",
    "    \n",
    "    #RandomForest parameter\n",
    "    'bag__random_state':[0],   \n",
    "    'bag__n_estimators' : [200,300],\n",
    "    'bag__max_features' : [1000,2000],\n",
    "    'bag__max_samples' : [2000],\n",
    "    'bag__base_estimator' : [DecisionTreeClassifier()]\n",
    "}\n",
    "    \n",
    "build_model(\"BAG\", True)\n",
    "display(models[\"BAG\"][\"gridsearch\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8a38d",
   "metadata": {},
   "source": [
    "*Both models are overfitting. The tuned model has performed much better in all metrics compared to the model with default parameter, looks like defining our hyperparameters worked positively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b494456",
   "metadata": {},
   "source": [
    "### 4.6 CountVectorizer and RandomForest\n",
    "***RandomForest*** uses ensemble method – combine multiple decision trees to predict an outcome.. \n",
    "\n",
    "Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each split in the learning process, a random subset of the features. This process is sometimes called the random subspace method.\n",
    "\n",
    "The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be used in many/all of the bagged decision trees, causing them to become correlated. By selecting a random subset of features at each split, we counter this correlation between base trees, strengthening the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "build_model(\"RF\", withGridSearch=False)\n",
    "display(models[\"RF\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747b126",
   "metadata": {},
   "source": [
    "#### Hyperparameters - RandomForestClassifier (from sklearn)\n",
    "\n",
    "***n_estimators***, default=100\n",
    "The number of trees in the forest.\n",
    "\n",
    "***max_depth***, default=None\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters\n",
    "\n",
    "models[\"RF\"][\"params\"]  ={\n",
    "\n",
    "    #RandomForest parameter\n",
    "    'rf__n_estimators' : [50, 75, 100],\n",
    "    'rf__max_depth' : [None, 1, 2,4,6],\n",
    "    'rf__random_state': [0]\n",
    "    \n",
    "}\n",
    "\n",
    "build_model(\"RF\", True)\n",
    "display(models[\"RF\"][\"gridsearch\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4583c6",
   "metadata": {},
   "source": [
    "*Both RandomForest models were also showing overfitting, there is no improvement in the model's score after tuning with hyperparameters.\n",
    "\n",
    "*Bagging Classifier seems to get better score than RandomForest.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6186f",
   "metadata": {},
   "source": [
    "### 4.7 CountVectorizer and ADA Boost\n",
    "***Boosting*** is a method of converting weak learners into strong learners.  Boosting takes a weak base learner and tries to make it a strong learner by retraining it on the misclassified samples.\n",
    "\n",
    "ADA Boost or Adaptive Boosting helps combine multiple weak classifier into a single strong classifier.\n",
    "It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "build_model(\"ADA\", withGridSearch=False)\n",
    "display(models[\"ADA\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a6be0",
   "metadata": {},
   "source": [
    "#### Hyperparameters - AdaBoostClassifier (from sklearn)\n",
    "\n",
    "***base_estimator*** default=None\n",
    "The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
    "\n",
    "***n_estimators***, default=50\n",
    "The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "***learning_rate***, default=1.0\n",
    "Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1984cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters\n",
    "models[\"ADA\"][\"params\"]  = {  \n",
    "\n",
    "    #ADA Boosting parameter\n",
    "    'ada__n_estimators' : [500,1000,4000],\n",
    "    'ada__learning_rate' : [0.8, 0.5, 0.2, 0.1],\n",
    "    'ada__base_estimator__max_depth' : [1, 2], \n",
    "    'ada__random_state': [0]\n",
    "}\n",
    "\n",
    "build_model(\"ADA\", True)\n",
    "display(models[\"ADA\"][\"gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6489dbe",
   "metadata": {},
   "source": [
    "*The model using GridSearch is performing a much better than the default parameter. Overfitting is minimized in the model with hyperparameters*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b71b6",
   "metadata": {},
   "source": [
    "### 4.8 CountVectorizer and Gradient Boost\n",
    "- In ***gradient boosting***, it trains many model sequentially. Each new model gradually minimizes the loss function (y = ax + b + e, e needs special attention as it is an error term) of the whole system using Gradient Descent method. The learning procedure consecutively fit new models to provide a more accurate estimate of the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "build_model(\"GB\", withGridSearch=False)\n",
    "display(models[\"GB\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbfc09",
   "metadata": {},
   "source": [
    "#### Hyperparameters - GradientBoostClassifier (from sklearn)\n",
    "\n",
    "\n",
    "***learning_rate***, default=0.1\n",
    "Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "***n_estimators***, default=100\n",
    "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "\n",
    "***max_depth***, default=3\n",
    "The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99987fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters\n",
    "\n",
    "models[\"GB\"][\"params\"]  = {\n",
    "    \"gb__max_depth\" : [4, 6,8],\n",
    "    \"gb__n_estimators\" : [500,600,800],\n",
    "    \"gb__learning_rate\" : [0.8, 0.10, 0.12],\n",
    "    \"gb__random_state\": [0]\n",
    "}\n",
    "\n",
    "\n",
    "build_model(\"GB\", True)\n",
    "display(models[\"GB\"][\"gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbc1ff",
   "metadata": {},
   "source": [
    "*The model using GridSearch is performing much better than the default parameter. Overfitting is also a problem in Gridsearch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d18a4",
   "metadata": {},
   "source": [
    "### 4.9 VotingClassifier\n",
    "A ***Voting Classifier*** also trains on an ensemble of numerous models and predicts an output based on their highest probability of chosen class as the output.\n",
    "\n",
    "It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n",
    "\n",
    "Hard Voting: In hard voting, the predicted output class is a class with the highest majority of votes\n",
    "Soft Voting: In soft voting, the output class is the prediction based on the average of probability given to that class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "X_train_cvec = cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)\n",
    "vote = VotingClassifier([\n",
    "    (\"tree\", DecisionTreeClassifier(random_state= 0)),\n",
    "    ('ada', AdaBoostClassifier(learning_rate= 0.1, n_estimators= 1000,  random_state= 0)),\n",
    "    ('gb', GradientBoostingClassifier(learning_rate =  0.12, max_depth = 6,n_estimators = 500,random_state= 0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models[\"VC\"][\"best_params_\"] = vote.best_params_\n",
    "\n",
    "models[\"VC\"][\"no_gridsearch\"] = {}\n",
    "models[\"VC\"][\"no_gridsearch\"][\"train_score\"] = vote.score(X_train_cvec, y_train)\n",
    "models[\"VC\"][\"no_gridsearch\"][\"test_score\"] = vote.score(X_test_cvec, y_test)\n",
    "models[\"VC\"][\"no_gridsearch\"][\"model\"] = vote\n",
    "\n",
    "predictions = models[\"VC\"][\"no_gridsearch\"][\"model\"].predict(X_test_cvec)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "sensitivity = tp / (tp + fn) \n",
    "specificity = tn / (tn + fp)\n",
    "precision =  tp / (tp + fp)\n",
    "f1_score = 2 * ((precision * sensitivity) / (precision + sensitivity))\n",
    "    \n",
    "    \n",
    "models[\"VC\"][\"no_gridsearch\"][\"sensitivity\"] = sensitivity\n",
    "models[\"VC\"][\"no_gridsearch\"][\"specificity\"] = specificity\n",
    "models[\"VC\"][\"no_gridsearch\"][\"precision\"] =  precision\n",
    "models[\"VC\"][\"no_gridsearch\"][\"f1_score\"] = f1_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(models[\"VC\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132014b",
   "metadata": {},
   "source": [
    "If we use voting = \"soft\", we can use the .proba to check the probability, but the default parameter \"hard\" performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b807fd3",
   "metadata": {},
   "source": [
    "### 4.99 CountVectorizer and SVM\n",
    "***Support Vector Machine*** algorithm separates data points using a hyperplane with the highest amount of margin.The algorithm determines the best decision boundary between vectors that belong to a given category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build with default parameter\n",
    "build_model(\"SVC\", withGridSearch=False)\n",
    "display(models[\"SVC\"][\"no_gridsearch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SVC()\n",
    "sv.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb840ff",
   "metadata": {},
   "source": [
    "#### Hyperparameters - AdaBoostClassifier (from sklearn)\n",
    "\n",
    "***C***, default=1.0\n",
    "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "\n",
    "***kernel*** {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’\n",
    "Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n",
    "\n",
    "***max_iter***, default=-1\n",
    "Hard limit on iterations within solver, or -1 for no limit.\n",
    "\n",
    "***gamma*** {‘scale’, ‘auto’} or float, default=’scale’\n",
    "Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning with hyperparameters\n",
    "#C is a penalty parameter, which represents misclassification or error term. \n",
    "#Gamma is a parameter that influences the calculation of plausible line of separation for SVM. \n",
    "#When gamma is higher, nearby points will have high influence in the calculation of the decision boundary. \n",
    "#A low gamma means far away points also be considered when calculating the decision boundary.\n",
    "#Kernel -?\n",
    "\n",
    "models[\"SVC\"][\"params\"]  = {  \n",
    "    #SVC Boosting parameter\n",
    "    'svm__max_iter' : [-1, 10_000],\n",
    "    'svm__C' : [0.0001, 0.1, 0.2, 0.3],\n",
    "    'svm__gamma':[0.1, 0.3], \n",
    "    'svm__kernel':['linear','rbf'],\n",
    "    \n",
    "}\n",
    "\n",
    "build_model(\"SVC\", True)\n",
    "display(models[\"SVC\"][\"gridsearch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983eb653",
   "metadata": {},
   "source": [
    "* There is slight improvement from the default model vs the tuned model, both models have overfitting problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dacc1",
   "metadata": {},
   "source": [
    "## 5.0 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bdf76",
   "metadata": {},
   "source": [
    "### 5.1 Score or Accuracy\n",
    "\n",
    "***Score or Accuracy means the count the number of matches (prediction vs actual) divided by by the number of samples.***\n",
    "\n",
    "*In this metrics, our top  model(s) within 82%:*\n",
    "- GB Test Score: 0.8269\n",
    "- BAG Test Score: 0.8237\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d464b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models by Score\n",
    "for model in models:\n",
    "    if model != 'VC':\n",
    "        print(f\"{model} Test Score: {models[model]['gridsearch']['test_score']}\")\n",
    "    else:\n",
    "        print(f\"{model} Test Score: {models[model]['no_gridsearch']['test_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936664e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_list = []\n",
    "\n",
    "for model in models:\n",
    "    if model != 'VC':\n",
    "        dic = {\"model\": model,\n",
    "                \"score\": models[model]['gridsearch'][\"test_score\"]\n",
    "        }\n",
    "        score_list.append(dic)\n",
    "df_score = pd.DataFrame(score_list)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8a389",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_score = df_score.sort_values(by=\"score\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.lineplot(data=df_score, x='model', y='score', ax=ax, marker='X');\n",
    "\n",
    "plt.title(\"Classification Models by Accuracy(Test Score)\", fontsize=12)\n",
    "#plt.ylim(0,0.9)\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b8f23",
   "metadata": {},
   "source": [
    "***BaggingClassifier and GradientBoost have the highest accuracy***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ec8bd",
   "metadata": {},
   "source": [
    "### 5.2 OTHER  METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6af8e",
   "metadata": {},
   "source": [
    "### Sensitivity\n",
    "\n",
    "***Sensitivity*** is a measure of the proportion of actual positive cases that got predicted as positive (or true positive). Sensitivity is also termed as Recall.\n",
    "\n",
    "In this project, our ***True Positive*** are those Nutrition that are accurately predicted as Nutrition.  ***False Negative*** means those Nutrition that are predicted as Keto.\n",
    "\n",
    "Sensitivity = (True Positive)/(True Positive + False Negative)\n",
    "\n",
    "The higher the Sensitivity Rate means we have captured more on our positive class (Nutrition)\n",
    "\n",
    "*In this metric, our best model is:*\n",
    "- TVEC Sensitivity: 0.8875   (88% of Nutrition are predicted accurately)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models by sensitivity\n",
    "for model in models:\n",
    "    if model != 'VC':\n",
    "        print(f\"{model} Sensitivity Rate: {models[model]['gridsearch']['sensitivity']}\")\n",
    "    else:\n",
    "        print(f\"{model} Sensitivity Rate: {models[model]['no_gridsearch']['sensitivity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ee425",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "\n",
    "***Specificity*** is a measure of the proportion of actual negative cases that got predicted as negative (or true negative). \n",
    "\n",
    "In this project, our ***True Negative*** are those Keto titles that are accurately predicted as Keto. ***False Positive*** means those Keto that are predicted as Nutrition title.\n",
    "\n",
    "Specificity = (True Negative)/(True Negative + False Positive)\n",
    "\n",
    "The higher the Specificity Rate means we have captured more accurately on our negative class (Keto)\n",
    "\n",
    "*In this metric, our best models are:*\n",
    "- BAG Specificity Rate: 0.835  (82% of Keto post are predicted accurately as Keto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models by specificity\n",
    "for model in models:\n",
    "    if model != 'VC':\n",
    "        print(f\"{model} Specificity Rate: {models[model]['gridsearch']['specificity']}\")\n",
    "    else:\n",
    "        print(f\"{model} Specificity Rate: {models[model]['no_gridsearch']['specificity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3272aa",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "***Precision***  evaluates the fraction of correctly classified instances or samples among the ones classified as positives.\n",
    "\n",
    "Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly:\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "*In this metric, our best models are:*\n",
    "- BAG Precision: 0.8315\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ded8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models by precision\n",
    "for model in models:\n",
    "    if model != 'VC':\n",
    "        print(f\"{model} Precision: {models[model]['gridsearch']['precision']}\")\n",
    "    else:\n",
    "        print(f\"{model} Precision: {models[model]['no_gridsearch']['precision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc29bb9",
   "metadata": {},
   "source": [
    "### F1_Score\n",
    "\n",
    "***F-score or F1 Score*** is a measure of a prediction's accuracy. It is calculated from the precision and recall/sensitivity of the test.\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall.\n",
    "If you care about precision and recall roughly the same amount, F1 score is a great metric to use.\n",
    "\n",
    "F1_Score = 2 * ((precision * sensitivity) / (precision + sensitivity))\n",
    "\n",
    "\n",
    "*In this metric, our best model is:*\n",
    "- GB F1 Score: 0.834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ba11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models by f1-score\n",
    "for model in models:\n",
    "    if model != 'VC':\n",
    "        print(f\"{model} F1 Score: {models[model]['gridsearch']['f1_score']}\")\n",
    "    else:\n",
    "        print(f\"{model} F1 Score: {models[model]['no_gridsearch']['f1_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab6c0b",
   "metadata": {},
   "source": [
    "## 6. Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af417365",
   "metadata": {},
   "source": [
    "### 6.1 Overfitting Problem\n",
    "\n",
    "***Almost all models have overfitting problems, Train Score has  more than 10% of the Test Score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overfit_list = []\n",
    "\n",
    "for model in models:\n",
    "    if model != 'VC':\n",
    "        dic = {\"model\": model,\n",
    "                          \"Score Diff\": models[model]['gridsearch']['train_score'] - models[model]['gridsearch']['test_score']\n",
    "                         }\n",
    "        overfit_list.append(dic)\n",
    "\n",
    "df_overfit = pd.DataFrame(overfit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overfit = df_overfit.sort_values(by=\"Score Diff\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.barplot(data=df_overfit, y='model',x='Score Diff', ax=ax, orient = \"h\", palette=[\"blue\",\"blue\",\"blue\",\"gray\",\"gray\",\"gray\",\"gray\",\"gray\", \"gray\"]);\n",
    "\n",
    "plt.title(\"Overfitting (Train Score - Test Score)\", )\n",
    "#plt.xlim(0,0.4)\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936927c",
   "metadata": {},
   "source": [
    "***DTREE, ADA, TVDC has below 10% difference from their Train Score, while the rest have difference between 12 to 20%%***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e63df",
   "metadata": {},
   "source": [
    "\n",
    "# 7.0 How do we choose our model then? What metrics should we consider to make the decision?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['test_score', 'sensitivity', 'f1_score']\n",
    "\n",
    "to_df = []\n",
    "\n",
    "for model in models:\n",
    "    if model in ('BAG', 'GB'):\n",
    "        for metric in metrics:\n",
    "\n",
    "            dic = {\"model\": model,\n",
    "                          \"metric\": metric,\n",
    "                          \"value\": models[model]['gridsearch'][metric]\n",
    "                         }\n",
    "            to_df.append(dic)\n",
    "df_model = pd.DataFrame(to_df)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607430fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.barplot(data=df_model, x='metric', y='value', hue='model', ax=ax, palette=[\"gray\", \"blue\"]);\n",
    "\n",
    "plt.title(\"GradientBoost Metrics - Accuracy, Sensitivity and F1 Score\", fontsize=12)\n",
    "plt.ylim(0,0.9)\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['specificity', 'precision']\n",
    "\n",
    "to_df = []\n",
    "\n",
    "for model in models:\n",
    "    if model in ('BAG', 'GB'):\n",
    "        for metric in metrics:\n",
    "\n",
    "            dic = {\"model\": model,\n",
    "                          \"metric\": metric,\n",
    "                          \"value\": models[model]['gridsearch'][metric]\n",
    "                         }\n",
    "            to_df.append(dic)\n",
    "df_model = pd.DataFrame(to_df)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.barplot(data=df_model, x='metric', y='value', hue='model', ax=ax, palette=[\"blue\", \"gray\"]);\n",
    "\n",
    "plt.title(\"BaggingClassifier Metrics - Specificity and Precision\", fontsize=12)\n",
    "plt.ylim(0,0.9)\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206bc643",
   "metadata": {},
   "source": [
    "***Accuracy***\n",
    "- GradientBoost:  83%\n",
    "\n",
    "***Sensitivity***\n",
    "- GradientBoost: 87% of Nutrition are predicted accurately (Positive Class)\n",
    "\n",
    "***Specificity***\n",
    "- BaggingClassifier: 84% of Keto post are predicted accurately as Keto (Negative Class)\n",
    "TFIDFVectorizer \n",
    "\n",
    "***Precision***\n",
    "- BaggingClassifier: 83% \n",
    "\n",
    "***F1 Score*** Balance between Sensitivity and Specificity\n",
    "- GradientBoost: 83%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da3534",
   "metadata": {},
   "source": [
    "# 7.1 MODEL SELECTION:\n",
    "\n",
    "We selected  **Bagging Classifier** as our best  model for this classification project, though ***GradientBoosting*** Classifier tops Accuracy and F1, it wasn’t that much difference. \n",
    "\n",
    "The major difference is in Sensitivity and Specificity, we want our negative class to be predicted more accurately, as *Keto* is strict diet and should not be classified under the *Nutrition* subreddit. Bagging Classifier is highest in this metric 83.5% compared to Bagging Classifier with 78.5\n",
    "\n",
    "I think the distribution of probabilities made a lot of difference too in this decision. As we can further enhance the Sensitivity of the BaggingClassifier by adjusting our decision boundaries. There is a trade-off between getting the best result and the interpretability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c7988",
   "metadata": {},
   "source": [
    "### 8.0 AUC-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9753c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize ROC\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "plot_roc_curve(models[\"BAG\"][\"gridsearch\"][\"model\"], X_test, y_test,  ax=ax, name='Bagging Classifier', color='green');\n",
    "plot_roc_curve(models[\"GB\"][\"gridsearch\"][\"model\"], X_test, y_test,  ax=ax, name='Gradient Boosting', color='blue');\n",
    "plot_roc_curve(models[\"TVEC\"][\"gridsearch\"][\"model\"], X_test, y_test,  ax=ax, name='TVEC & LogisticRegression', color='gray');\n",
    "\n",
    "\n",
    "plt.title('Top 3 Models (AUC-ROC)')\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf5d40",
   "metadata": {},
   "source": [
    "## 9.0 POST MODEL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20db4b4",
   "metadata": {},
   "source": [
    "### 9.1 Distribution of True Value (Y value) and the Prediction (probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1413275",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = models[\"BAG\"][\"gridsearch\"][\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7674cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bg = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs': bg.predict_proba(X_test)[:,1]})\n",
    "\n",
    "pred_bg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a169ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "color=[\"orange\", \"green\"]\n",
    "i = 0\n",
    "# plot distributions of predicted probabilities by actual values\n",
    "for group in pred_bg.groupby('true_values'):\n",
    "    sns.distplot(group[1], kde = False, bins = 20, label = f'Actual Outcome = {group[0]}', color=color[i])\n",
    "    i += 1\n",
    "\n",
    "# Add cutoff line\n",
    "plt.axvline(0.5, color='black', linestyle='--')\n",
    "\n",
    "plt.title(\"Bagging Classifier MODEL\")\n",
    "plt.xlabel('Predicted Probability that Outcome = 1');\n",
    "plt.legend(loc=\"center right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5838d",
   "metadata": {},
   "source": [
    "The orange ones on the left are the Actual post for Keto. If there is no overlap with greens, those are the True Negatives, the model correctly classified as Keto.\n",
    "\n",
    "The green on the right are the Actual post for Nutrition.If there is no overlap with green, those are the True Positives, where the model correctly classified as Nutrition.\n",
    "\n",
    "The ovelap however are the False Positives (the green overlapping with orange that goes beyond 50%),  and the False Negatives (the orange that overlaps the green below 50%) .\n",
    "\n",
    "*You will notice that the distribution in the middle is around the mean, as Bagging is ensemble technique and the score is the average from multiple models. We can also adjust our decision boundary for example to .4 (or probability of 40%) to capture more True Positives increasing our Sensitivity Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = models[\"GB\"][\"gridsearch\"][\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tv = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs': gb.predict_proba(X_test)[:,1]})\n",
    "\n",
    "pred_tv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cce2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "\n",
    "color=[\"orange\", \"green\"]\n",
    "i = 0\n",
    "\n",
    "# plot distributions of predicted probabilities by actual values\n",
    "for group in pred_tv.groupby('true_values'):\n",
    "    sns.distplot(group[1], kde = False, bins = 20, label = f'Actual Outcome = {group[0]}', color=color[i])\n",
    "    i += 1\n",
    "\n",
    "# Add cutoff line\n",
    "plt.axvline(0.5, color='black', linestyle='--')\n",
    "\n",
    "plt.xlabel('Predicted Probability that Outcome = 1')\n",
    "plt.title(\"GradientBoost Classifier\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d41ee4",
   "metadata": {},
   "source": [
    "*The misclassifications are spread out for GradientBoost. There are many orangecrossing the decision boundary and missclassified as Nutrition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc12a48",
   "metadata": {},
   "source": [
    "### 9.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ae8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the confusion matrix for GradientBoost\n",
    "plot_confusion_matrix(models['GB']['gridsearch']['model'], X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1615f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = models['GB']['gridsearch']['model'].predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662faf5f",
   "metadata": {},
   "source": [
    "###### ***Interpretation of the confusion matrix for GradientBoost***\n",
    "\n",
    "- True Negative (tn) -> 376 (Actual = Keto  and Predicted = Keto)\n",
    "- False Positive (fp) - > 103 (Actual = Keto  and Predicted = Nutrition)\n",
    "- False Negative (fn) -> 63 (Actual = Nutrition  and Predicted = Keto)\n",
    "- True Positive  (tp) -> 417 (Actual = Nutrition  and Predicted = Nutrition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the confusion matrix for BaggingClassifier\n",
    "plot_confusion_matrix(models['BAG']['gridsearch']['model'], X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d883241",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = models['BAG']['gridsearch']['model'].predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618b173",
   "metadata": {},
   "source": [
    "***Interpretation of the confusion matrix for Bagging***\n",
    "\n",
    "- True Negative (tn) -> 400 (Actual = Keto  and Predicted = Keto)\n",
    "- False Positive (fp) - > 79 (Actual = Keto  and Predicted = Nutrition)\n",
    "- False Negative (fn) -> 90 (Actual = Nutrition  and Predicted = Keto)\n",
    "- True Positive (tp) -> 390 (Actual = Nutrition  and Predicted = Nutrition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e426e",
   "metadata": {},
   "source": [
    "### 10.0 Deep dive in misclassified posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ce73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's build a dataframe to look closer of our original data including the model's prediction\n",
    "##USING BAGGING CLASSIFIER MODEL\n",
    "\n",
    "predictions = pd.DataFrame(bg.predict_proba(X))\n",
    "predictions.rename(columns={0: 'Keto', 1: 'Nutrition'}, inplace=True)\n",
    "predictions['title'] = df_subr['title']\n",
    "\n",
    "#our actual label\n",
    "predictions['actual'] = df_subr['is_nutrition']\n",
    "\n",
    "#the model's prediction\n",
    "predictions['predict'] = gb.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all misclassified post\n",
    "misclass_preds = predictions[ (predictions['actual']!= predictions['predict']) ]\n",
    "misclass_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdaa8b6",
   "metadata": {},
   "source": [
    "####  False Negative -- posted in \"Nutrition\" but model wrongly predicted as \"Keto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8206ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's got some samples with over 80% probability\n",
    "false_neg = misclass_preds[(misclass_preds['Keto'] > 0.70) & (misclass_preds['actual'] == 1)]\n",
    "\n",
    "false_neg['actual'] = false_neg['actual'].map(lambda x: 'Nutrition' if x==1 else 'Keto')\n",
    "false_neg['predict'] = false_neg['predict'].map(lambda x: 'Nutrition' if x==1 else 'Keto')\n",
    "\n",
    "false_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7eff4f",
   "metadata": {},
   "source": [
    "***I could understand the wrong prediction, the \"keto\" is all over the title, and probably the reason it was predicted as negative class***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a952d99f",
   "metadata": {},
   "source": [
    "####  False Positives -- posted in \"Nutrition\" but model wrongly predicted as \"Keto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff4c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets get some sample data\n",
    "false_pos = misclass_preds[(misclass_preds['Nutrition'] > 0.70) & (misclass_preds['actual'] == 0)]\n",
    "\n",
    "false_pos['actual'] = false_pos['actual'].map(lambda x: 'Nutrition' if x==1 else 'Keto')\n",
    "false_pos['predict'] = false_pos['predict'].map(lambda x: 'Nutrition' if x==1 else 'Keto')\n",
    "\n",
    "false_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b341842",
   "metadata": {},
   "source": [
    "#### 'Nutrients, Vitamins, Protein' are key words for Nutrition, so they are predicted as Nutrition though they posted under Keto subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0d8b4",
   "metadata": {},
   "source": [
    "### Coef_\n",
    "\n",
    "Bagging/Boosting doesn't have coef so we will use TVEC. This is one of the trade-off for using more complex models, the interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use the model for the TFIDFVecotizer to get the coefficients related to Nutrition and Keto\n",
    "coef = pd.DataFrame(models[\"TVEC\"][\"gridsearch\"][\"model\"].best_estimator_.steps[1][1].coef_).T\n",
    "coef.columns = ['coef']\n",
    "coef['ngram'] = models[\"TVEC\"][\"gridsearch\"][\"model\"].best_estimator_.steps[0][1].get_feature_names()\n",
    "coef = coef[['ngram','coef']]\n",
    "coef = coef.sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbe73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nutrition_coef = coef.head(15).reset_index(drop=True)\n",
    "top_keto_coef = coef.tail(15).sort_values(by='coef', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aad990",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.barplot(data=top_nutrition_coef, x='coef', y='ngram', palette='Greens', ax=ax);\n",
    "\n",
    "ax.set_title(\"Top 15 Ngrams Correlated with Nutrition\", fontsize =12)\n",
    "ax.set_ylabel('nutrition - word(s)',fontsize=10)\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.barplot(data=top_keto_coef, x=abs(top_keto_coef['coef']), y='ngram', palette='Oranges', ax=ax);\n",
    "\n",
    "ax.set_title(\"Top 15 Ngrams Correlated with Keto\", fontsize =12)\n",
    "ax.set_ylabel('keto - word(s)',fontsize=10)\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75e118",
   "metadata": {},
   "source": [
    "# 12.0 Summary:\n",
    "\n",
    "***To answer our Business Problem: Yes, we have created models and selected the best one that can classify posts from two different subreddits based on the TITLE with 83% Accuracy.***\n",
    "     \n",
    "- We selected  ***Bagging Classifier*** as our best  model for this classification project, though GradientBoosting Classifier tops Accuracy and F1, it wasn’t that much difference. The major difference is in Sensitivity and Specificity, we want our negative class to be predicted more accurately, as Keto is strict diet and should not be classified under the Nutrition subreddit. Bagging Classifier is highest in this metric with 83.5% rate compared to Bagging Classifier with only 78.5%\n",
    "     \n",
    "## CHALLENGES\n",
    "- We have observed overfitting in most of the models. This limitation was not overcome by ensemble methods and probably need more tuning on the hyperparameters to achieve better results.\n",
    "\n",
    "- Time to run GridSearch using hyperparameters.  Boosting is slowest amongst all models, as it builds the model sequentially instead of parallel as Random Forest and Bagging, so tuning the hyperparameters can take longer time, 10 minutes if you minimize your hyperparameters, but can take hours if you add more parameters.\n",
    "    \n",
    "# Recommendation:\n",
    "  - Tuning of hyperparameters to overcome overfitting\n",
    "  - Include other features like self-text and probably sentiment analysis score might improve our metrics.\n",
    "  - Include images or videos in our anaysis for more accurate prediction (which requires more knowledge on different ML domains)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e50a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
